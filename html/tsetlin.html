<html>
<head>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" href="https://latex.now.sh/style.css"> 
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>

</head>

<body id="top">
  <header>
    <h1>Deterministic Tsetlin Machines</h1>
    <p class="author">Jonny Edwards  <br> May 2020</p>
  </header>
  
<div class="abstract">
  <h5>Abstract</h5>
  <p>Tsetlin Machines and randomness. Here we attempt to remove some of the random elements of the algorithm
  with a goal of
  a more deterministic process.</p>
</div>

<h1> Introduction </h1>

<p> <a href="https://arxiv.org/abs/1804.01508">The Tsetlin Machine</a> (TM) is a novel
Machine Learning (ML) architecture which is particularly suited to low
power and explainable applications. Since the algorithm itself is built
using existing logic building blocks, there is the potential to build
learning systems at the edge with a fraction of the hardware utilised by
existing Deep Learning (DL) methods, with the same level of performance.
This paper describes an initial practical assessment, which deliberately
focuses on the area of hyper-parameter tuning and use of randomness
which can be particularly costly energy wise.</p>

<p> The paper is constructed as follows, firstly, we describe our
interpretation of the TM algorithm, with reference to the learning
procedure presented in the originating paper. We then explain
modifications to the algorithm to test aspects of the initialisation
particularly related to controlling the amount of randomness. In this
section we describe an approximation, which takes the further step of
removing some of the necessity for randomness. We test this together
with hyper-parameter selection on three well known classification
data-sets, using cross-validated test set accuracy. Finally, we discuss
the results and future work. </p>

<h1> Algorithm Description </h1>

<p> The underlying structure of the TM is the same as the majority of
learning classification algorithms with forward pass inference and a
backward pass parameter adjusting learning step. A key feature of the
algorithm is that the whole system is Boolean, requiring an encoder and
decoder (explained more in Figure below)
 for any other form of data. Data is defined as the
set of vectors: </p>

$$D_n= \{x_1,x_2,...,x_n\}$$

<p> And output is: </p>

$$Y = f(x;T)$$

<p> Where Y is a "class" output \((Y_{Cl}= \{y_1,y_2,...,y_{cl}\})\) The
system is discriminatory (\(p(Y|D=x)\)) not generative (\(p(D|Y=y)\). The 
process of learning is the optimisation of Tsetlin automata
parameters \(t\) (labelling the full collection \(T\) such that the
function provides the most accurate estimate of a given set of classes
\(Y\). The inference mechanism is an ensemble of disjunctive clauses, each
clause controlling the inclusion of a feature and (separately) its
negation. Formally a clause (\(c_{jcl}\)is the $j$th clause for class
\(cl\) with \(J\) the total number of classes) is:</p>

$$c_j= \bigwedge\limits^2F_{i=0}(x^i \land a^i) \land (\neg x^{2i} \land a^{2i})$$

<p> Where \(x^i\) is the \(ith\) feature of vector \(x\) and \(F\) is the total
number of features and \(a^i\) is defined below. A variety of ensembling
methods can be used - most commonly single voted threshold, with half
the clauses aimed at inhibition: </p>

$$\label{ex}y = 1   \quad if  \quad (\sum_{j=0}^{J/2} c_{j0} -\sum_{j=j/2+1}^Jc_{j0} ) \ge 0  \quad  else  \quad 0$$

<p> Where the algorithm is multi-class (<a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoded</a> thresholding
is removed and the maximum vote is used to assess the estimated class:
</p>

$$y = y_{cl} \; if \; max(\sum_{j=0}^{J/2}c_{jcl} - \sum_{j=(j/2)+1}^Jc_{jcl} ) \quad \forall \;  y_{cl} \;  in \: Y_{Cl}$$

<p> The learning mechanism controls the inclusion of features or their
negation and uses classic <a href="https://books.google.co.uk/books?id=sgDHJlafMskC&pg=PA284&lpg=PA284&dq=tsetlin+russian&source=bl&ots=pc9p4aQEcg&sig=ACfU3U1cNtadI8FKW2-AbbkKOWBMXCPx5Q&hl=en&sa=X&ved=2ahUKEwjTzujUtPDpAhVNZhUIHdGqARsQ6AEwBHoECA0QAQ#v=onepage&q=tsetlin%20russian&f=false">Tsetlin automata</a> binary action (a),
which exist as thresholdable integer count values. Formally, a Tsetlin
automata (\(t^i\) is: \(a^i=1 \: if \: t^i > t_{thres} \: else \: 0\) A
reward for a Tsetlin automata is \(t^i+=1\) whilst a penalty is \(t^i-=1\).
The learning rules are described in detail in <a href="">this paper</a>, 
the basic premise being that
individual automata can be controlled by examining their effect on the
clause output relative to the desired output (\(y_{est}$\)), the clause
output, the polarity (an inhibitory or excitory clause) and the action
(\(a^i\)). For example, if a feature is \(1\) and the Tsetlin (\(t\)) is
resolved to action (\(a \land x = 1\)) and the overall clause is
inhibiting when an 0 output is desired, then the Tsetlin is rewarded
(+1), as it is trying to do the right action. </p>

<p> The learning approach has the following added randomness:</p>

<ol>
<li> An annealing style cooling parameter $Temp$, which we set to \(15\) in
    this paper, and have found has little effect.</li>

<li>   Sensitivity \(S\) which controls the ratio of reward to penalty, based
    on a split of the total probability, with values \(1/S\) and \(1-1/S\).
    This is used in a *choice* function on a tuple \((penalty,reward)\) as
    shown in Table in the Appendix. </li>
</ol>

<p> Again, for further background and justification see
<a href="https://arxiv.org/abs/1804.01508">here</a>.</p>

<h2> Comparisons with MLP based learning </h2>

<p> It is worthwhile to compare the TMs with a classical <a href='https://en.wikipedia.org/wiki/Multilayer_perceptron'>MLP</a>
; noteworthy points include: Boolean/integer rather than the floating
point internally; logic rather than linear algebra; and binary
inputs/outputs.</p>


<p> The backwards pass requires no gradient methods (just manipulation of
all Tsetlin states (T)). Due to the negation of features, clauses can
singularly encode non linearly separable boundaries. There are no
sigmoid style functions, all functions are compatible with low level
hardware implementations. "Weights" (Tsetlin Automata) in the TM are
thresholded not "neurons" (clauses).

<p> TMs and <a href="https://arxiv.org/abs/1804.01508">Binary Neural Networks</a> (BNNs) share many similarities, most
notably the resultant inference mechanism being purely logic based.
However, the major contrast is that TMs eradicate non-Boolean values
during training by encoding the non-Boolean inputs and decoding at the
output phase, whereas BNNs transform the entire internal representation
after training.</p>

<p> TMs vs BNN's: A TM (1) carries it's encoding and decoding strategy
into the inference process, where ANN adapts it's internal weight via
<a href='https://nervanasystems.github.io/distiller/quantization.html'>quantisation</a>.</p>

<h2>Modifications to Randomness</h2>

<p> To assess the sensitivity to stochasticity discussed above, we produce a
series of random samples at different cycle lengths - the numbers are
generated from a uniform distribution over a partitioned interval, so,
for a two cycle array random numbers are generated in the range \(0-0.49\)
and \(0.5-1\) and then shuffled. As a further example, for a ten cycle
array random numbers are generated in the random array:</p>

$$ \{0 < r_1 \leq 0.09,0.1\leq r_2 \leq 0.19,0.2 \leq r_3 \leq 0.29, $$

$$ 0.3\leq r_4 \leq 0.39,0.4 \leq r_5 \leq 0.49,0.5 \leq r_6 \leq 0.59, $$
$$  0.6 \leq r_7 \leq 0.69,0.7 \leq r_8 \leq 0.79, 0.8 \leq r_9 \leq 0.89, $$
$$ 0.9 \leq r_{10} \leq 0.99\} $$ 
which is then shuffled.

<h2> Deterministic Learning </h2>


<p> The algorithm can be further modified to replace the stochastic
expectation with a guaranteed reinforcement signal, replacing long-range
expectations with single update values. We rely on the fact that at
every epoch of training the learning effect can be approximated by the
instantaneous effect of the penalty and reward combined. Mathematically,
if we assume that the final state (\(F\)) is a sum collection (over a time
period \(T\)) of updates (\(u\)) and this is a collection of penalties (\(p\))
and rewards (\(r\)):</p>

$$F \pm \sum^T_{t=0}u_t \approx F \pm\sum^T_{t=0}|r_t-p_t|$$

<p> This removes the requirement for choice between reward and penalty at
the cost of a non 1/-1 based accumulation, with the implication being
float point based Tsetlin Automata units. In practice we have
experimented with reward/penalty combinations that produce integer
values, and will report this in a later paper. </p>

<h2> Empirical Evaluation </h2>


<p> This section describes how we have assessed the practicalities of this
algorithm, paying particular attention to the practicalities of
establishing the correct hyper-parameters. </p>

<h3> Experimental Design </h3>


<p> We take three data-sets, the classic <em>Iris</em>  data-set, <em>Digit</em> and
<em>Breast</em>, and apply best practice 10-fold cross validation over a
selection of hyper-parameters and then report out of sample train/test
set error. For reference, <a href="https://en.wikipedia.org/wiki/XGBoost">XGBoost</a> test set
performance is included (implemented via the <a href="https://arxiv.org/abs/1804.01508">scikit-learn</a> python library).</p>

<h4> Data-sets </h4>

<p> The emphasis of this experimentation is to establish the ability to work
in the same domain as the <em>XGBoost</em> algorithm, with a bias towards
smaller samples (rather than DL style large data-sets). The data-sets
are readily available (in `sklearn`) standard machine learning problems.
The Table below  gives the
python function call to access the data. Training was not batched. </p>

<p> Due to the Boolean nature of the algorithm and the ranges of the
features, we transformed any float or integer into a unary "thermometer"
code based on a 5 bit discretisation across the dynamic range (<b>1 bit
step</b> = \((max(x^i)-min(x^i))/5)\).</p>
<br><br>
<table>
<thead>
<th>  Data-set </th> <th> Size </th> <th> Feat,Class </th> <th> sklearn command</th>
</thead>
<tbody>
<tr> <td>  Iris   </td> <td> 150 </td> <td> 4<em>Flt</em>, 3cls </td> <td> `load_iris()`</td>
</tr> <tr> <td>  Breast </td> <td> 569 </td> <td>30<em>Flt</em>, 2cls       </td> <td> `load_breast_cancer() `</td>
</tr> <tr> <td>  Digit  </td> <td> 1797</td> <td> 64 <em>4bit Int</em>, 10cls </td> <td> `load_digits()`</td></tr> 
</tbody>
</table>
<br>


<p> Data-sets used in evaluation, <em>Size</em> is the number of examples in
  the data-set.</p>

<h4> Hyper-parameter Settings </h4>

<p> The following hyper-parameters exist for the learning algorithm:</p>

<table>
<thead><th> Variable </th> <th>  Settings</th></thead> 
<tbody><tr><td>  RType    </td> <td>  [LR,HR,D]</td></tr> 
<tr><td>  cl       </td> <td>  [10,20,50,100,200]</td></tr> 
<tr><td>  it       </td> <td>  [20,50,100,200]</td></tr> 
<tr><td>  S        </td> <td>  [2,3.9,6]</td></tr></tbody>
</tbody>
</table>

<p> Iterations (it) - the maximum number of epochs for learning
 update, Randomisation Type, (RType), Number of clauses (cl)
</p>
<br><br>

<table>
<thead><th>  Randomisation (Rtype) </th> <th>   cycle size </th> </thead>

<tbody> <tr><td>  Low Random (LR)     </td><td>        100
</td></tr><tr><td>  High Random (HR)    </td><td>        1000
</td></tr><tr><td>  Deterministic (D)   </td><td>         \-
</td></tr></tbody>
</table>

<h2> Results </h2>

<p> The tables below present the
experimental results, for brevity we include the top 10 results, which
cover best results for all the algorithm apart from the ranked 15th
result for the <em>D</em> on <em>Digit</em>. </p>

<h3>Iris</h3>
<table>
<thead><th>   RType</th> <th>   Cl</th> <th>    Iter </th> <th>   S  </th> <th>  Trn/mean </th> <th>     Trn/std </th> <th>    Tst/mean </th> <th>  Tst/std </th> </thead>
<tbody>
<tr><td>        </td><td>        </td><td>        </td><td>      </td><td>      </td><td>  <em>XGBoost</em> </td><td> <em>0.9543</em></td><td>  0.022 </td><tr>
<tr><td>     D  </td><td>   100 </td><td>   20  </td><td>  3.9 </td><td>   0.972 </td><td>  0.003 </td><td>       0.961   </td><td>  0.021 </td><tr>
<tr><td>     D  </td><td>   100 </td><td>  100  </td><td>  3.9 </td><td>   0.976 </td><td>  0.005 </td><td>       0.960   </td><td>  0.023 </td><tr>
<tr><td>     D  </td><td>   100 </td><td>   50  </td><td>  3.9 </td><td>   0.974 </td><td>  0.003 </td><td>       0.959   </td><td>  0.024 </td><tr>
<tr><td>    HR  </td><td>   50  </td><td>  200  </td><td>   2  </td><td>   0.968 </td><td>  0.003 </td><td>       0.959   </td><td>  0.024 </td><tr>
<tr><td>     D  </td><td>   100 </td><td>   20  </td><td>   6  </td><td>   0.971 </td><td>  0.004 </td><td>       0.957   </td><td>  0.030 </td><tr>
<tr><td>    LR  </td><td>   100 </td><td>   50  </td><td>  3.9 </td><td>   0.973 </td><td>  0.004 </td><td>       0.957   </td><td>  0.023 </td><tr>
<tr><td>    HR  </td><td>   200 </td><td>  200  </td><td>   6  </td><td>   0.987 </td><td>  0.004 </td><td>       0.955   </td><td>  0.028 </td><tr>
<tr><td>     D  </td><td>   200 </td><td>  200  </td><td>   6  </td><td>   0.988 </td><td>  0.004 </td><td>       0.955   </td><td>  0.026 </td><tr>
<tr><td>    LR  </td><td>   200 </td><td>  200  </td><td>   6  </td><td>   0.988 </td><td>  0.004 </td><td>       0.955   </td><td>  0.034 </td><tr>
</tbody>
</table>
<br><br>

<!-- </td></tr><tr><td>  : Top 10 sorted by Test set error - `Iris` data-set results. The -->
<!-- </td></tr><tr><td>  **`XGB`** row is the **`XGBoost`** results when applied to this -->
<!-- </td></tr><tr><td>  data-set.[]{label="t1"} -->   
<h3>Breast</h3>
<table>
<thead><th>   RType</th> <th>   Cl</th> <th>    Iter </th> <th>   S  </th> <th>  Trn/mean </th> <th>     Trn/std </th> <th>    Tst/mean </th> <th>  Tst/std </th> </thead>
<tbody>
<tr><td>       </td><td>       </td><td>       </td><td>        </td><td>        </td><td>         <em>XGBoost</em> </td><td> 0.927</td><td> 0.08 </td></tr> 
<tr><td>    HR </td><td>    200 </td><td>  100  </td><td>   6   </td><td>  0.979 </td><td>         0.004 </td><td>     0.9268  </td><td>   0.062 </td></tr>
<tr><td>    LR </td><td>    100 </td><td>  200  </td><td>  3.9  </td><td>  0.974 </td><td>         0.006 </td><td>     0.9267  </td><td>   0.069 </td></tr>
<tr><td>     D </td><td>    100 </td><td>   20  </td><td>   6   </td><td>  0.962 </td><td>         0.014 </td><td>       0.92  </td><td>   0.071 </td></tr>
<tr><td>     D </td><td>    100 </td><td>  100  </td><td>  3.9  </td><td>  0.968 </td><td>         0.005 </td><td>       0.92  </td><td>   0.071 </td></tr>
<tr><td>     D </td><td>    200 </td><td>  100  </td><td>   6   </td><td>  0.974 </td><td>         0.007 </td><td>       0.92  </td><td>   0.071 </td></tr>
<tr><td>    HR </td><td>    200 </td><td>  200  </td><td>   6   </td><td>  0.977 </td><td>         0.006 </td><td>       0.92  </td><td>   0.057 </td></tr>
<tr><td>     D </td><td>    100 </td><td>   50  </td><td>  3.9  </td><td>  0.973 </td><td>         0.006 </td><td>       0.92  </td><td>   0.065 </td></tr>
<tr><td>     D </td><td>    100 </td><td>   50  </td><td>   6   </td><td>  0.963 </td><td>         0.005 </td><td>       0.92  </td><td>   0.083 </td></tr>
<tr><td>     D </td><td>    100 </td><td>  100  </td><td>   6   </td><td>  0.961 </td><td>         0.008 </td><td>       0.92  </td><td>   0.083 </td></tr>
<tr><td>     D </td><td>    100 </td><td>  200  </td><td>  3.9  </td><td>  0.965 </td><td>         0.008 </td><td>       0.92  </td><td>   0.065 </td></tr>
</tbody>
</table>
<br><br>

  <!-- : Top 10 sorted by Test set error - `Breast` data-set -->
  <!-- results.[]{label="t2"} -->
<h3>Digit</h3>
<table>
<thead><th>   RType</th> <th>   Cl</th> <th>    Iter </th> <th>   S  </th> <th>  Trn/mean </th> <th>     Trn/std </th> <th>    Tst/mean </th> <th>  Tst/std </th> </thead>
<tbody>
<tr><td>             </td><td>      </td><td>       </td><td>       </td><td>            </td><td>      <em>XGBoost</em></td><td>      0.966  </td><td>    0.008</td></tr>
<tr><td>      LR     </td><td>  20  </td><td>   20  </td><td>   2   </td><td>     0.976  </td><td>     0.005 </td><td>      0.959  </td><td>   0.023 </td></tr>
<tr><td>      LR     </td><td>  10  </td><td>  200  </td><td>   2   </td><td>     0.982  </td><td>     0.004 </td><td>      0.954  </td><td>   0.020 </td></tr>
<tr><td>      HR     </td><td>  10  </td><td>  100  </td><td>   6   </td><td>     0.981  </td><td>     0.006 </td><td>      0.952  </td><td>   0.030 </td></tr>
<tr><td>      LR     </td><td>  20  </td><td>   50  </td><td>  3.9  </td><td>     0.967  </td><td>     0.005 </td><td>      0.948  </td><td>   0.027 </td></tr>
<tr><td>      LR     </td><td>  20  </td><td>  200  </td><td>   6   </td><td>     0.971  </td><td>     0.005 </td><td>      0.948  </td><td>   0.027 </td></tr>
<tr><td>      HR     </td><td>  20  </td><td>   50  </td><td>   2   </td><td>     0.972  </td><td>     0.003 </td><td>      0.948  </td><td>   0.031 </td></tr>
<tr><td>      LR     </td><td>  200 </td><td>  100  </td><td>  3.9  </td><td>     0.982  </td><td>     0.005 </td><td>      0.946  </td><td>   0.019 </td></tr>
<tr><td>      HR     </td><td>  50  </td><td>   50  </td><td>  3.9  </td><td>     0.982  </td><td>     0.005 </td><td>      0.946  </td><td>   0.019 </td></tr>
<tr><td>      HR     </td><td>  50  </td><td>  100  </td><td>   6   </td><td>     0.982  </td><td>     0.005 </td><td>      0.946  </td><td>   0.019 </td></tr>
<tr><td>      LR     </td><td>  200 </td><td>  200  </td><td>   6   </td><td>     0.964  </td><td>     0.005 </td><td>      0.945  </td><td>   0.026 </td></tr>
<tr><td>   <em>...15</em> D </td><td>  200 </td><td>  200  </td><td>  3.9  </td><td>     0.989  </td><td>     0.003 </td><td>      0.942  </td><td>   0.024 </td></tr>
</tbody>
</table>

  <!-- : Top 10 sorted by Test set error - `Digit` data-set results. Note -->
  <!-- that the highest deterministic (D) result is include at position 15 in -->
  <!-- the table.[]{label="t3"} -->

<h2> Discussion and Further Work </h2>

<p> The following observations are noteworthy:</p>

<ul>
<li> In general TMs have low variance between training runs - training is
    a fairly stable process given a set of parameters.</li>

<li>   <b> <em>XGBoost</em></b> -  which is excellent, considering that this is
    thought to be the best known "out-the-box" classifier.</li>

<li> <b> D </b> - Surprisingly, the deterministic algorithm features highly in
    two of the three data-sets, with strong performance in the <em>Breast</em>
    data. Also, it seems this algorithm requires less training than the
    LR, HR perhaps due to a more predictable reinforcement given the
    learning signal. </li>

<li>  <b> LR </b> - Low Randomness features strongly in the evaluation with
    best performing test-set error in <em>Digit</em> and <em>Breast</em> data-set
    evaluations. </li>

<li>  <b> HR </b> - High Randomness does not feature as readily as
    anticipated - it is clearly the case that limiting the entropy
    within the learning process does not unduly affect overall test set
    accuracy.</li>

<li>  <b> Class size </b> - In general there was a spread of class sizes,
    surprisingly the `Digit` problem features low class (20) for the
    best performing.</li>

<li>  <b> Iterations </b> - For each data-set, there is always a TM, trained
    over less than 50 iterations, within 2% of <em>XGBoost</em> test set
    accuracy. There was also a variety of iteration sizes represented in
    the top ten of each, with no real bias towards higher iterations. It
    appears that the deterministic (D) algorithm can be run for less
    iterations than the random algorithms.</li>

<li> <b>S</b> - Again the full range of \(S\) are represented in the best
    results, although there is perhaps a bias towards the training
    cycles using \(6\).</li>
</ul>

<p> In summary, this algorithm, especially in it's low and deterministic
form, offers a viable method of building a classifier, with a stable
simple learning, and unlike BNNs no post training quantisation process.</p>

<p> For future work we are particularly interested in how this might
transfer to hardware, as unlike MLPs the operations are all standard
logic operations and the method can be readily parallelised at the
clause level. Furthermore, the clauses form compact logical
representations that are amenable to further analysis to gain a stronger
understanding of the internalised knowledge. Clearly, if the algorithm
can be successfully applied to the latest "Deep" data-sets, then there
is huge potential to build explainable and fast hardware
implementations. </p>

<h4> appendix </h4>
<pre>
   y   y   polarity   clause   literal x   exc/inc   (re,in,pen)
  --- --- ---------- -------- ----------- --------- -------------
   1   0    0 (-1)      0          0          0        (L,B,0)
   1   0    0 (-1)      0          0          1        (0,B,L)
   1   0    0 (-1)      0          1          0        (L,B,0)
   1   0    0 (-1)      0          1          1        (0,B,L)
   1   0    0 (-1)      1          0          0        (L,B,0)
   1   0    0 (-1)      1          0          1           0
   1   0    0 (-1)      1          1          0        (0,L,B)
   1   0    0 (-1)      1          1          1        (B,L,0)
   1   0      1         0          0          0           0
   1   0      1         0          0          1           0
   1   0      1         0          1          0           0
   1   0      1         0          1          1           0
   1   0      1         1          0          0        (0,0,1)
   1   0      1         1          0          1           0
   1   0      1         1          1          0           0
   1   0      1         1          1          1           0
   0   1    0 (-1)      0          0          0           0
   0   1    0 (-1)      0          0          1           0
   0   1    0 (-1)      0          1          0           0
   0   1    0 (-1)      0          1          1           0
   0   1    0 (-1)      1          0          0        (0,0,1)
   0   1    0 (-1)      1          0          1           0
   0   1    0 (-1)      1          1          0           0
   0   1    0 (-1)      1          1          1           0
   0   1      1         0          0          0        (L,B,0)
   0   1      1         0          0          1        (0,B,L)
   0   1      1         0          1          0        (L,B,0)
   0   1      1         0          1          1        (0,B,L)
   0   1      1         1          0          0        (L,B,0)
   0   1      1         1          0          1           0
   0   1      1         1          1          0        (0,L,B)
   0   1      1         1          1          1        (1,0,0)

 Update lookup where $L=1/S$ $B= S/S-1$ as per Tsetlin paper.
 Polarity is whether the clause is excitory or inhibitory in Equation
</pre>

</body>
</html>
